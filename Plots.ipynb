{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Loading Image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [07:30<00:00,  2.14s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'gt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-94fc344900df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figures/big_img.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mcurve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cochlear_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/DataStorage/Dropbox (Partners HealthCare)/HairCellInstance/src/functional.py\u001b[0m in \u001b[0;36mget_cochlear_length\u001b[0;34m(image, equal_spaced_distance, diagnostics)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'gt'"
     ]
    }
   ],
   "source": [
    "from src.models.HCNet import HCNet\n",
    "from src.utils import calculate_indexes, remove_edge_cells\n",
    "import src.functional\n",
    "from src.cell import cell\n",
    "\n",
    "import skimage.io as io\n",
    "import torch\n",
    "import click\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "default_path = '/media/DataStorage/ToAnalyze/' \\\n",
    "               'Jul 23 Control m2.lif - TileScan 1 Merged.tif'\n",
    "\n",
    "print('Loading Model...')\n",
    "model = torch.jit.script(HCNet(in_channels=3, out_channels=4, complexity=15)).cuda()\n",
    "model.load_state_dict(torch.load('Dec_17_REALLY_GOOD.hcnet'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# (Z, Y, X, C)\n",
    "print('Loading Image...')\n",
    "image_base = io.imread(default_path)\n",
    "base_im_shape = image_base.shape\n",
    "\n",
    "\n",
    "\n",
    "out_img = torch.zeros((1, image_base.shape[2], image_base.shape[1], image_base.shape[0]), dtype=torch.int16) # (1, X, Y, Z)\n",
    "\n",
    "max_cell = 0\n",
    "\n",
    "x_ind = calculate_indexes(25, 613, base_im_shape[2], base_im_shape[2])\n",
    "y_ind = calculate_indexes(25, 613, base_im_shape[1], base_im_shape[1])\n",
    "total = len(x_ind) * len(y_ind)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (x, y) in tqdm(product(x_ind, y_ind), total=total):\n",
    "\n",
    "        image = torch.from_numpy(image_base[:, y[0]:y[1], x[0]:x[1], [0, 2, 3]] / 2 ** 16).unsqueeze(0)\n",
    "        image = image.transpose(1, 3).transpose(0, -1).squeeze().unsqueeze(0).sub(0.5).div(0.5).cuda()\n",
    "\n",
    "        if image.max() == -1:\n",
    "            continue\n",
    "\n",
    "        out = model(image.float().cuda(), 5)\n",
    "        prob_map = torch.sigmoid(out[:, -1, ...]).unsqueeze(1)\n",
    "        out = out[:, 0:3:1, ...]\n",
    "        embed = src.functional.vector_to_embedding(out)\n",
    "        centroids = src.functional.estimate_centroids(embed, 0.001, 40)\n",
    "\n",
    "\n",
    "        if centroids.nelement() == 0:\n",
    "            del image, out, centroids\n",
    "            continue\n",
    "\n",
    "        out = src.functional.embedding_to_probability(out, cent, torch.tensor([0.006]))\n",
    "        print(cent.shape[1])\n",
    "\n",
    "        \n",
    "        value, out = out.max(1)\n",
    "        out[prob_map[:,0,...] < 0.5]=0\n",
    "\n",
    "        max_cell = out.max()\n",
    "        out = out[..., 0:image.shape[-1]].cpu().to(out_img.dtype)\n",
    "\n",
    "        # post processing\n",
    "        out = remove_edge_cells(out)\n",
    "        # out = remove_small_cells(out)\n",
    "\n",
    "        out_img[:, x[0]:x[1]-1, y[0]:y[1]-1:, 0:image.shape[-1]-1][out != 0] = out[out != 0]\n",
    "\n",
    "        del image, out, cent, value\n",
    "        \n",
    "\n",
    "torch.save(out_img, 'out_image.trch')\n",
    "out_img = out_img.squeeze(0).int().numpy().transpose((2,1,0))\n",
    "io.imsave('figures/big_img.tif', out_img)        \n",
    "\n",
    "curve, percent, apex = src.functional.get_cochlear_length(out_img > 0, 10)\n",
    "print(curve.shape)\n",
    "\n",
    "plt.imshow(out_img[0,...].gt(0).sum(-1).gt(3))\n",
    "plt.plot(curve[0,:], curve[1,:], 'r')\n",
    "plt.show()\n",
    "\n",
    "cells = []\n",
    "for u in tqdm(torch.unique(out_img)):\n",
    "    cells.append(cell(image_base.unsqueeze(0), (out_img == u).unsqueeze(0)))\n",
    "\n",
    "torch.save(cells, 'cells.trch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img =  torch.load ('out_image.trch')\n",
    "print(out_img.max())\n",
    "out_img = out_img.squeeze(0).float().numpy().transpose((2,1,0))\n",
    "io.imsave('figures/big_img.tif', out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.HCNet_legacy import HCNet\n",
    "from src.utils import calculate_indexes, remove_edge_cells\n",
    "import src.functional\n",
    "import skimage.io as io\n",
    "import torch\n",
    "import click\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "default_path = '/media/DataStorage/ToAnalyze/' \\\n",
    "               'Jul 23 Control m2.lif - TileScan 1 Merged.tif'\n",
    "    \n",
    "image_base = io.imread(default_path)\n",
    "out_img = torch.load('out_image.trch')\n",
    "curve, percent, apex = src.functional.get_cochlear_length(out_img > 0, .5)\n",
    "print(curve.shape)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(image_base.max(0)[:,:,[1,2,3]] / 2**16)\n",
    "plt.plot(curve[1,:], curve[0,:], 'r', linewidth = 5)\n",
    "plt.savefig('curveature.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(image_base.max(0)[:,:,[1,2,3]] / 2**16)\n",
    "plt.plot(curve[1,:], curve[0,:], 'r', linewidth = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base.max(0).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watershed Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.segmentation\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image = io.imread('/media/DataStorage/Dropbox (Partners HealthCare)/HairCellInstance/data/'\\\n",
    "                  'test/C2-Jul-1-AAV2-PHP.B-CMV3-m2.lif---m2.og.tif')\n",
    "image.shape\n",
    "segments_watershed = skimage.segmentation.watershed(image[..., 3], markers=500, compactness=20)\n",
    "\n",
    "plt.imshow(skimage.segmentation.mark_boundaries(segments_watershed[22,...], image[22,:,:,[0,2,3]]))\n",
    "io.imsave('watershed.tif',segments_watershed)\n",
    "plt.imshow(skimage.segmentation.mark_boundaries(segments_watershed[22,...], image[22,:,:,[0,2,3]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import src.dataloader\n",
    "import src.transforms as t\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print('Loading Images...')\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    t.save_image('figures/0_base_image'),\n",
    "    t.nul_crop(rate=1),\n",
    "    t.save_image('figures/1_nul_crop'),\n",
    "    t.random_crop(shape=(256, 256, 16)),\n",
    "    t.save_image('figures/2_random_crop'),\n",
    "    t.elastic_deformation(grid_shape=(3, 3, 2), scale=1.5),\n",
    "    t.save_image('figures/3_elastic_deformation'),\n",
    "    t.to_cuda(),\n",
    "    t.random_h_flip(rate=1),\n",
    "    t.save_image('figures/4_horizontal_flip'),\n",
    "    t.random_v_flip(rate=1),\n",
    "    t.save_image('figures/5_vertical_flip'),\n",
    "    t.random_affine(shear=(-15, 15)),\n",
    "    t.save_image('figures/6_random_affine'),\n",
    "    t.adjust_brightness(range_brightness = (-0.2, 0.2)),\n",
    "    t.save_image('figures/7_adjust_brightness'),\n",
    "    t.adjust_gamma(),\n",
    "    t.save_image('figures/8_adjust_gamma'),\n",
    "    t.adjust_centroids(),\n",
    "])\n",
    "data = src.dataloader.dataset('/media/DataStorage/Dropbox (Partners HealthCare)/HairCellInstance/data/validate', transforms=transforms)\n",
    "dl = DataLoader(data, batch_size=1, shuffle=False, num_workers=0)\n",
    "print('Done')\n",
    "\n",
    "_ = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.cell\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cells = torch.load('cells.trch')\n",
    "\n",
    "gfp = []\n",
    "dapi = []\n",
    "myo = []\n",
    "actin = []\n",
    "volume = []\n",
    "\n",
    "for c in cells:\n",
    "    gfp.append(c.gfp.item())\n",
    "    dapi.append(c.dapi.item())\n",
    "    myo.append(c.myo7a.item())\n",
    "    actin.append(c.actin.item())\n",
    "    volume.append(c.volume.item())\n",
    "    \n",
    "myo = torch.tensor(myo)\n",
    "gfp = torch.tensor(gfp)\n",
    "dapi = torch.tensor(dapi)\n",
    "actin = torch.tensor(actin)\n",
    "volume = torch.tensor(volume)\n",
    "\n",
    "plt.hist(gfp[myo > 0.05].numpy(), bins=20,color = 'g')\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlabel('GFP Cell Intensity')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dapi[myo > 0.05].numpy(), bins=20,color = 'b')\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlabel('DAPI Cell Intensity')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(myo[myo > 0.05].numpy(), bins=20, color = 'y')\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlabel('DAPI Cell Intensity')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(volume[myo > 0.05].numpy(), bins=20, color = 'k')\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlabel('DAPI Cell Intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import skimage.io as io\n",
    "\n",
    "rcdnet = io.imread('/media/DataStorage/Dropbox (Partners HealthCare)/mini-CMV project/Jul 18 AAV2-PHP.B-CMV4 m2.lif - TileScan 1 Merged_segmentation.tif')\n",
    "unet = io.imread('/media/DataStorage/ToAnalyze/Jul 18 AAV2-PHP.B-CMV4 m2.lif - TileScan 1 Merged_cellBycell/test_unqiue_mask.tif')\n",
    "\n",
    "rcdnet = torch.from_numpy(rcdnet)\n",
    "unet = torch.from_numpy(unet)\n",
    "\n",
    "unique, counts = torch.unique(rcdnet, return_counts=True)\n",
    "plt.hist(counts.numpy())\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.xlabel('DAPI Cell Intensity')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = torch.unique(rcdnet, return_counts=True)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(counts[unique != 0].numpy(), bins=40,alpha=0.5)\n",
    "\n",
    "\n",
    "unique, counts = torch.unique(unet, return_counts=True)\n",
    "plt.hist(counts[unique != 0].numpy(), bins=40,alpha=0.5)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.legend(['HCNet','UNet'])\n",
    "plt.xlabel('Cell Volumes')\n",
    "plt.axvline(5000, color='k')\n",
    "plt.axvline(15000, color='k')\n",
    "plt.savefig('volumes.png',dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
